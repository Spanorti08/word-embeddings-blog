[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "outline.html",
    "href": "outline.html",
    "title": "Blog Outline",
    "section": "",
    "text": "How machines pick up vibes from text\n\n\n\nJordan is a second-year undergraduate student majoring in computer science. He has done introductory coursework in Python and machine learning, but has no prior exposure to natural language processing or word embedding models. Jordan wants to understand how modern applications like chatbots and search engines interpret language because he is considering taking more applied AI courses next year. His goal is to gain an intuitive understanding of how words can be converted into numbers without diving into graduate-level mathematics.\n\n\n\n\nIntroduction\n\nWhy language is difficult for machines\nMotivation for embeddings\n\nWhat are word embeddings?\n\nBasic definition\nWhy they are meaningful to computers\n\nWhy do we need them?\n\nLimitations of old methods (one-hot, bag-of-words)\n\nHow do embeddings work\n\nThe idea of vector space\nSimilarity + simple arithmetic examples\n\nApplications in real systems\n\nSearch\nChatbots\nRecommenders\nTranslation\n\nConclusion\n\nKey takeaways\nWhy embeddings matter in modern NLP"
  },
  {
    "objectID": "outline.html#title",
    "href": "outline.html#title",
    "title": "Blog Outline",
    "section": "",
    "text": "How machines pick up vibes from text"
  },
  {
    "objectID": "outline.html#audience-persona",
    "href": "outline.html#audience-persona",
    "title": "Blog Outline",
    "section": "",
    "text": "Jordan is a second-year undergraduate student majoring in computer science. He has done introductory coursework in Python and machine learning, but has no prior exposure to natural language processing or word embedding models. Jordan wants to understand how modern applications like chatbots and search engines interpret language because he is considering taking more applied AI courses next year. His goal is to gain an intuitive understanding of how words can be converted into numbers without diving into graduate-level mathematics."
  },
  {
    "objectID": "outline.html#structure",
    "href": "outline.html#structure",
    "title": "Blog Outline",
    "section": "",
    "text": "Introduction\n\nWhy language is difficult for machines\nMotivation for embeddings\n\nWhat are word embeddings?\n\nBasic definition\nWhy they are meaningful to computers\n\nWhy do we need them?\n\nLimitations of old methods (one-hot, bag-of-words)\n\nHow do embeddings work\n\nThe idea of vector space\nSimilarity + simple arithmetic examples\n\nApplications in real systems\n\nSearch\nChatbots\nRecommenders\nTranslation\n\nConclusion\n\nKey takeaways\nWhy embeddings matter in modern NLP"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/word-embeddings/index.html",
    "href": "posts/word-embeddings/index.html",
    "title": "How machines pick up vibes from text",
    "section": "",
    "text": "Introduction\nLanguage feels easy for humans, but computers don’t actually “get” what words mean. For a long time, the best they could do was treat text like a bunch of labels, which works fine for counting words but not for understanding them. Modern apps like search, chatbots, and translators obviously do better than that, so there has to be a trick behind the scenes. One of the biggest tricks is something called word embeddings. They’re a way to turn words into numbers that actually reflect meaning. In this blog, we’ll look at what that means in plain language and why it’s useful.\n\n\nWhat are word embeddings?\nWord embeddings are basically a way to turn words into numbers, but not random numbers. The numbers are chosen so that words with similar meanings end up close together. For example, “cat” and “dog” might be near each other, while “banana” sits somewhere completely different. Once words live in this space, computers can start doing useful things with them, like comparing how similar two words are. The important part is that embeddings capture meaning from how words are used in real text, not from a dictionary. So instead of treating words as labels, embeddings give computers a way to work with language in a more natural way. \n\n\nWhy do we need them?\nBefore embeddings were used, computers mostly represented words as separate “tokens.” A common approach was something called one-hot encoding, where each word became a long list of zeros with a single one. This makes it impossible for a computer to know that “cat” and “dog” are related, because their vectors share nothing in common. Techniques like bag-of-words also ignored word meaning and only cared about how often words showed up. These old methods work okay for basic tasks, but they fall apart when you want computers to understand relationships between words. Embeddings fix this by letting similar words have similar numbers. That simple change makes language tasks like search, translation, and chatbots way less painful for machines.\n\n\nHow do they work in practice?\nThe easiest way to understand embeddings is to look at how they place words in a bigger “space.” Imagine every word gets an address, and words that show up in similar contexts get similar addresses. For example, if you scan a lot of text and notice that “cat” and “dog” both appear near words like “pet,” “fur,” and “walk,” the embedding model naturally places them near each other. A famous demonstration is the “king — man + woman = queen” example. It sounds like magic at first, but what’s really happening is that the model picks up patterns in how words relate to each other. Because everything turns into numbers, you can do little math tricks that reveal relationships. This makes computers way better at handling language tasks without needing dictionaries or hand-written rules.\n\n\n\nExample of king-man+woman=queen. Image generated by author using ChatGPT.\n\n\nAnother helpful way to picture this is to imagine sorting objects around your room. You might group your headphones, phone charger, and keyboard together because they’re tech items, while snacks and drinks naturally end up in a different pile. You don’t need anyone to explain that logic to you — it’s obvious from how you use those things. Embeddings work in a similar way. The model reads tons of real text and learns which words tend to “hang out” together. Words that belong to similar topics slowly move closer to each other, while unrelated words drift apart. Over time, this creates a map where you can ask cool questions like “What words are closest to coffee?” or “What’s between teacher and student?” The answers won’t always be perfect, but they often make sense in a very human way. This hidden structure is what allows bigger language models to feel like they understand meaning, even though they’re just playing with numbers.\n\n\nApplications in real systems\nWord embeddings show up in a lot of places, even if you never hear the term. Search engines use them to match your query with pages that talk about similar ideas, not just the exact words you typed. Chatbots and large language models rely on embeddings to follow your conversation and generate responses that make sense. Recommendation systems use embeddings to connect people with products, articles, or videos that “feel” related. Translation tools rely on embeddings to line up words and phrases across languages. Even autocomplete on your phone depends on embeddings to guess what you might say next. The main point is that embeddings quietly let computers deal with meaning in a flexible way, which unlocks a ton of features we now take for granted.\nIf you look closer, you’ll notice that many apps basically boil down to “find things that are similar.” Embeddings give them a way to do that without relying on exact wording. For example, if you search for “how to fix my wifi,” you don’t only get pages with those exact words. You also get results about routers, internet problems, and connectivity issues because embeddings help the search engine connect the dots. Movie and music apps do the same thing, but with items instead of words. A system might learn that people who like one comedy movie also tend to like a specific actor or director, and it can recommend related content even if you’ve never searched for it.\nBigger language models use embeddings as a starting point to make sense of whatever you type. The embedding layer turns your text into numbers that the rest of the model can actually work with. Without that first step, the model would be stuck staring at raw text that means nothing to it. So even though embeddings sound like a small detail, they’re one of the building blocks that make modern AI tools feel usable, helpful, and sometimes surprisingly smart.\n\n\nConclusion\nWord embeddings give computers a shortcut for dealing with meaning. Instead of treating words as unrelated labels, embeddings turn them into numbers that reflect how they’re used in the real world. That small shift unlocks a lot of useful tricks, from better search results to chatbots that can actually hold a conversation. You don’t need to know the math behind embeddings to appreciate what they do — just knowing that words live in a “space” is enough to understand the idea. And now that embeddings exist, almost every modern NLP system relies on them in one form or another."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "word-embeddings-blog",
    "section": "",
    "text": "How machines pick up vibes from text\n\n\n\nnlp\n\nembeddings\n\nmachine-learning\n\n\n\nExplaining how word embeddings help computers understand language\n\n\n\n\n\nJan 17, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 17, 2026\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 14, 2026\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  }
]