# Blog Outline

## Title
How machines pick up vibes from text

## Audience Persona
Jordan is a second-year undergraduate student majoring in computer science. He has done introductory coursework in Python and machine learning, but has no prior exposure to natural language processing or word embedding models. Jordan wants to understand how modern applications like chatbots and search engines interpret language because he is considering taking more applied AI courses next year. His goal is to gain an intuitive understanding of how words can be converted into numbers without diving into graduate-level mathematics.

## Structure
1. Introduction
    - Why language is difficult for machines
    - Motivation for embeddings
2. What are word embeddings?
    - Basic definition
    - Why they are meaningful to computers
3. Why do we need them?
    - Limitations of old methods (one-hot, bag-of-words)
4. How do embeddings work
    - The idea of vector space
    - Similarity + simple arithmetic examples
5. Applications in real systems
    - Search
    - Chatbots
    - Recommenders
    - Translation
6. Conclusion
    - Key takeaways
    - Why embeddings matter in modern NLP
